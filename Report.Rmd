---
output: html_document
---
<span style="color:purple">A/B Testing</span>
========================================================
#### Udacity.com/ Data Analyst Nanodegree 11'2016
#### P7: Design an A/B Test

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
___________________
```{r, message=FALSE, warning=FALSE, packages,include=FALSE}
# Loading packages

library(ggplot2)
```

## <span style="color:purple">Experiment Design</span>


#### <span style="color:#596479">1. EXPERIMENT OVERVIEW: Free Trial Screener</span>

At the time of this experiment, Udacity courses currently have two options on the home page:

  1. "start free trial": The student will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. Metric __Number of Clicks__

  2. "access course materials": The student will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback. Metric __Number of Cookies__.
    
In the experiment, Udacity tested a change where if the student clicked "start free trial", they were asked how much time they had available to devote to the course:

  1. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. (Metric Number of user-ids)

  2. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. (Metric __Number of Cookies__)   
<br>

#### <span style="color:#596479">2. THE HYPOTHESIS</span>  

The hypothesis testing is to determine how likely it is that the results occured by chance, that is calculating the probability of results due to chance. The baseline is that there is no difference in control and experiment results.

For this project the null hypothesis  was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time—without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.

<br>

#### <span style="color:#596479">3. METRIC CHOICE</span>

The business objective is to help students get a job and be profitable in order to provide that service. Based on information available (the number of homepage visits, number of users explore the site, number of users who create an account and finally complete the course), I need to define the metrics for the experiment and decide how I need to measure if the experiment results are better than the control group results.  

<br>

##### 3.1 Invariant Metrics

Invariant metrics are the metrics that I expect to have the same values between our experimental and control groups. After randomly assigning subjects into experimental and control groups, invariant metrics let me verify that the division has been performed correctly. That is, the counts or rates are equivalent between groups.

The appropriate invariant metrics with corresponding practical significance boundaries (d_min) as absolute changes:
<br>

•	__Number of Cookies__: A number of unique cookies to view the course overview page, d_min = 3000. It is a unit of diversion, although if the student enrolls in the free trial, they are tracked by user-id from that point forward.
Cookies are explicitly randomized and used for dividing the total user population. I chose this metric, because cookies (pageviews) are counted before the users choose to click "start free trial" or "access course materials", that is before the experiment.

•	__Number of Clicks__: A number of unique cookies to click the "Start free trial" button, which happens before the free trial screener is trigger, d_min = 240. I choose this invariant metric because clicks are counted after the user clicks "start free trial", but before the screener, before they are exposed to the experiment. 

•	__Click-through-probability__: A number of unique cookies to click the "Start free trial" button divided by number of unique cookies to view the home page ( __Number of Clicks__ / __Number of Cookies__), d_min = 0.01. It measures the progression and provides more accurate information what portion of unique original visitors clicked the "Start free trial" button. I chose it, because it is counted before the user sees the experimental change, even if it is calculated using previously described invariant metrics.

•	__Number of user-ids __: Number of user-ids to enroll in the free trial that day (Enrollments). I did not selected this metric as invariant metric, since it would not be distributed evenly between both groups.

<br>

##### 3.2 Evaluation Metrics

Evaluations metrics are the metrics I expect to show the difference between our experimental and control groups. I am choosing a suite of metrics to see how they move.

•	__Gross Conversion__: A number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the "Start free trial" button. (__Number of user ids__ (Enrollments)/ __Number of Clicks__). The practical significance level, or the minimum difference we care about d_min = 0.01. I chose it, because it will let me see if the experiment had an effect on the number of users enrolling after they click. To reject the null hypothesis, this metric has to show statistically and practically significant decrease.

•	__Retention__: A number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (__Payments__ / __Number of user-ids__ ). The practical significance level d_min = 0.01. I chose it, because it lets me see how many users remained enrolled after the trial period. 

•	__Net Conversion__: A number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the "Start free trial" button. (__Payments__/ __Number of Clicks__). The practical significance level dmin = 0.0075. I chose this metric because the change in this metric lets me to determine if the experiment had an effect on the number of uses who clicked and became paying customers after the free trial ends. To reject the null hypothesis, this metric has to stay at constant level or increase, while __Gross Conversion__ would show practically significant decrease.

•	__Number of user-ids __: Number of user-ids to enroll in the free trial that day (Enrollments). I did not selected this metric as evalution metric, because it is incorporated in the 3 metrics listed above. Also, the size of the experiment and control groups might be slightly different. So, as a raw count, the number of user- ids might not be able to adjust to this such that it gives an accurate result.

<br>

#### <span style="color:#596479">4. MEASURING STANDARD DEVIATION</span>

The following table contains rough estimates of the baseline values for these metrics, given a sample size of 5000 cookies visiting the course overview page.

```{r, loading_data}
# Load the Data
options(scipen = 999)
setwd("/Users/daivasatas/Documents/DAND/P7/final/")
dat_sd <- read.csv("ABtesting_1.csv", header = F, check.names = F)
dat_sd
```

A formula to calculate the analytic estimate of standard deviation:
$$σ = \sqrt{\frac{p*(1-p)}{N}}$$

```{r, evaluation_metrics_SD_calculation}
clicks_sample <- 5000 * 0.08
user_id_sample <- clicks_sample * 0.20625

SD_GS <- round(sqrt(0.20625*(1-0.20625)/clicks_sample), digits = 4)
SD_R <- round(sqrt(0.53*(1-0.53)/user_id_sample), digits = 4)
SD_NC <- round(sqrt(0.1093125*(1-0.1093125)/clicks_sample), digits = 4)
```

```{r, evaluation_metrics_SD_printing}
cat(paste('Standard deviation for Gross Conversion: ', SD_GS))
cat(paste('Standard deviation for Retention: ', SD_R))
cat(paste('Standard deviation for Net Conversion: ', SD_NC))
```

The __Gross Conversion__ and __Net Conversion__ metrics are calculated using the __Number of Cookies__, which is the unit of diversion. I can expect the analytical estimate of variability to be comparable to the empirical variability so I don't need to calculate the empirical variability. For the __Retention__ metric I need to calculate the empirical variance because it is calculated using user-id as its denominator.

<br>

#### <span style="color:#596479">5. SIZING</span>

<br>

##### 5.1 Number of Samples vs. Power

Bonferroni Correction is not used in the calculations, since the __Gross Conversion__ and __Net Conversion__ metrics are correlated. 

To decide how many pageviews I need to get statistically significant result, which is statistical power. The statistical power has an inverse trade-off with size. The smaller the change we need to detect, or increased confidence we want to have in the result, we have to run larger experiment with more pageviews.

To calculate pageviews needed I used the code provided by Udacity A/B Testing course developers group.

```{r, source_empirical_sizing}
# credit to Udacity A/B Testing course developers group
source("empirical_sizing.R")
```

<br>

• The parameters to calculate the sample size for __Gross Conversion__:

    Base conversion rate = 0.20625    
    The minimum detectable effect  d_min = 0.01
    The statistical power, sensitivity 1-β = 0.8
    Confidence level = 95%
    The significance level, probability falsely concluding there was a difference (α) = 0.05

```{r, sample_size_gross_conversion}
GS_size <- required_size(s=sqrt(0.20625*(1-0.20625)*2), d_min=0.01, Ns=1:200000,
                         alpha=0.05, beta=0.2) * (40000 / 3200) * 2
cat(paste('Required number of pageviews: ', GS_size))
```

<br>

• The parameters to calculate the sample size for **Retention**:

    Base conversion rate = 0.53   
    The minimum detectable effect  d_min = 0.01
    The statistical power, sensitivity 1-β = 0.8
    Confidence level = 95%
    The significance level, probability falsely concluding there was a difference (α) = 0.05
    
```{r, retention_size}
R_size <- required_size(s=sqrt(0.53*(1-0.53)*2), d_min=0.01, Ns=1:200000,alpha=0.05, beta=0.2) * (40000 / 660) * 2
cat(paste('Required number of pageviews: ', round( R_size, digits = 0)))
```

<br>

• The parameters to calculate the sample size for __Net Conversion__:

    Base conversion rate = 0.1093125  
    The minimum detectable effect  d_min = 0.01
    The statistical power, sensitivity 1-β = 0.8
    Confidence level = 95%
    The significance level, probability falsely concluding there was a difference (α) = 0.05
    
```{r, net_conversion_size}
NC_size <- required_size(s=sqrt(0.1093125*(1-0.1093125)*2), d_min=0.0075, Ns=1:200000,alpha=0.05, beta=0.2) * (40000 / 3200) * 2
cat(paste('Required number of pageviews: ', NC_size))

```


<br>

##### 5.2 Duration vs. Exposure

The experiment would be considered high risk if someone could be harmed or when sensitive information is collected. Implementing a screaner to provide an expectation on time commitment I would consider low risk.  I think, it will not impact existing paying customers, and it is safe to run the test on 100% of user traffic.

```{r, experiment_duration_calculation}
cookies_per_day <- dat_sd[1,2]
duration_GC <- ceiling(GS_size / cookies_per_day)
duration_R <- ceiling(R_size / cookies_per_day)
duration_NC <- ceiling(NC_size / cookies_per_day)
```

```{r, duration_printing}
cat(paste('Length of experiment driven by Gross Conversion: ', duration_GC, 'days'))
cat(paste('Length of experiment driven by Retention: ', duration_R, 'days'))
cat(paste('Length of experiment driven by Net Conversion: ', duration_NC, 'days'))
```

The sample size for __Retention__ is above 4 mill, which means the experiment would last for almost 4 months. It is unacceptably long. If reducing the traffic, even to 50%, the duration is still to long to run an experiment. So I decided to remove __Retention__ as an evaluation metric. 

With given that the unique cookies to view page per day = 40000, the required pageviews of sample size for __Gross Conversion__  = 642475, length of the experiment = 16.06 days , which is rounded up to 17 days,  that it would give enough power and we can measure that part of a day. The required pageviews of sample size for __Net Conversion__  = 679300, length of the experiment = 16.98 days (rounded up to 17 days). These two measures are much more reasonable. 

To get power for all metrics the final number of page views must be the larger value between __Gross Conversion__ and __Net Conversion__, thus ensuring power for both metrics. So, the final pageview count is 679300, even if the lenth of the exeriment is the same (in days) for both __Gross Conversion__ and __Net Conversion__ metrics.


<br>

## <span style="color:purple">Experiment Analysis</span>

In the table below are a sample of the data to analyze. This data contains the raw information needed to compute the above metrics, broken down day by day.

```{r, Load_the_Data}
# Load the Data
control <- read.csv("Control.csv", header = T, check.names = F,
                    na.strings = c(""))

experiment <- read.csv("Experiment.csv", header = T, check.names = F,
                    na.strings = c(""))
```


```{r, data_sample}
head(control)
```

The meaning of each column is:

• Pageviews: Number of unique cookies to view the course overview page that day.

• Clicks: Number of unique cookies to click the course overview page that day.

• Enrollments: Number of user-ids to enroll in the free trial that day.

• Payments: Number of user-ids who enrolled on that day to remain enrolled for 14 days and thus make a payment. (Note that the date for this column is the start date, that is, the date of enrollment, rather than the date of the payment. The payment happened 14 days later. Because of this, the enrollments and payments are tracked for 14 fewer days than the other columns.)

<br>

#### <span style="color:#596479">6. SANITY CHECKS</span>

Checking count invariant metrics between the two groups:

<br>

• __Number of Cookies__:
```{r, Cookies_total}
Cookies_total <- sum(control$Pageviews) + sum(experiment$Pageviews)
cat(paste('Total number of cookies: ',Cookies_total))
```

```{r, SD_cookies_total}
options("scipen"= 100, "digits"=4)
SD_cookies_total = sqrt((0.5 * 0.5)/ Cookies_total)
cat(paste('Standard deviation: ', round(SD_cookies_total, digits = 4)))
```

```{r, m_cookies_total}
m_cookies_total <- 1.96 * SD_cookies_total
cat(paste('Margin of errors: ', round(m_cookies_total, digits = 4)))
```

```{r, CI_cookies_tota}
CI_cookies_total_low <- 0.5 - m_cookies_total
CI_cookies_total_high <- 0.5 + m_cookies_total
cat(paste('Confidence interval: [', round(CI_cookies_total_low,digits = 4),', ',round(CI_cookies_total_high,digits = 4),']'))
```

```{r, Fraction_control_cookies}
Fraction_control_cookies <- sum(control$Pageviews) / Cookies_total
cat(paste('Fraction of control Number of cookies: ', round(Fraction_control_cookies, digits = 4)))
```

The actual number for __Number of Cookies__ is within the confidence interval:

```{r, cookies_pass}
cookies_pass <- CI_cookies_total_low < Fraction_control_cookies & CI_cookies_total_high  > Fraction_control_cookies
cat(paste(cookies_pass))
```

The actual proportion of cookies for control group = 0.5006, which is inside the confidence interval, therefore the __Number of Clicks__ metric passes the sanity check.

<br>

• __Number of clicks__ :
```{r, Clicks_total}
Clicks_total <- sum(control$Clicks) + sum(experiment$Clicks)
cat(paste('Total number of clicks: ',Clicks_total))
```

```{r, SD_clicks_total}
SD_clicks_total = sqrt((0.5 * 0.5)/ Clicks_total)
cat(paste('Standard deviation: ', round(SD_clicks_total, digits = 4)))
```

```{r, margin_of_error_clicks_total}
m_clicks_total <- 1.96 * SD_clicks_total
cat(paste('Margin of error: ', round(m_clicks_total, digits = 4)))
```

```{r, CI_clicks_total}
CI_clicks_total_low <- 0.5 - m_clicks_total
CI_clicks_total_high <- 0.5 + m_clicks_total
cat(paste('Confidence interval: [', round(CI_clicks_total_low,digits = 4),', ',round(CI_clicks_total_high,digits = 4),']'))
```

```{r, Fraction_control_clicks}
Fraction_control_clicks <- sum(control$Clicks) / Clicks_total
cat(paste('Fraction of control Number of cookies: ', round(Fraction_control_clicks, digits = 4)))
```

The actual number for __Number of Clicks__ is within the confidence interval:
```{r, clics_pass}
clics_pass <- CI_clicks_total_low < Fraction_control_clicks & CI_clicks_total_high  > Fraction_control_clicks
cat(clics_pass)
```

The actual proportion of clicks for control group = 0.5005, which is inside the confidence interval, therefore the __Number of Clicks__ metric passes the sanity check.

<br>

• __Click-through-probability__ ( __Number of Clicks__ / __Number of Cookies__ ):
```{r}
Clicks_experiment <- sum(experiment$Clicks)
Cookies_experiment <- sum(experiment$Pageviews)
p_exp <- Clicks_experiment / Cookies_experiment

Clicks_control <- sum(control$Clicks)
Cookies_control <- sum(control$Pageviews)
p_control <- Clicks_control / Cookies_control

p_diff <- p_exp - p_control
p_hat <- Clicks_total/Cookies_total

cat(paste('Unit of diversion of control group: ', round(p_control,digits = 5)))
cat(paste('Unit of diversion of experiment group: ',round(p_exp,digits = 5)))
cat(paste('Pooled probability of a click: ',round(p_hat,digits = 4)))
SE_pooled <- sqrt(p_hat*(1-p_hat)*((1/Cookies_experiment)+(1/Cookies_control)))
Margin_error <- 1.96 * SE_pooled

cat(paste('Pooled standard error: ', round(SE_pooled, digits = 4)))
cat(paste('Margin of Error: ', round(Margin_error,digits = 4)))

CI_pooled_low <- p_control - Margin_error
CI_pooled_high <- p_control + Margin_error

cat(paste('CI (95%): [',round(CI_pooled_low, digits = 4) , ',',round(CI_pooled_high,digits = 4),']'))
```

The unit of diversion for experiment group is within the 95% confidence interval. 

• __Click-through-probability__ pases the sanity check:
```{r, CTP_pass}
CTP_pass <- CI_pooled_low < p_exp & p_exp  < CI_pooled_high
cat(CTP_pass)
```

<br>

#### <span style="color:#596479">7. RESULT ANALYSIS</span>

<br>

##### 7.1 Effect Size Tests

The evaluation metric is statistically significant if the confidence interval does not include 0, which indicates there was a change. It is practically significant if the confidence interval does not include the practical significance boundary, which means we can be confident there is a change that matters to the business.

<br>

• Confidence Interval for __Gross Conversion__: 
```{r, control_group}
control_clicks <- sum(control$Clicks[1:23]) 
control_enrollments <- sum(control$Enrollments[1:23])
control_payments <- sum(control$Payments[1:23])
```

```{r, experiment_group}
experiment_clicks <- sum(experiment$Clicks[1:23]) 
experiment_enrollments <- sum(experiment$Enrollments[1:23])
experiment_payments <- sum(experiment$Payments[1:23])
```

```{r}
clicks_paid <- control_clicks + experiment_clicks
enrollments_paid <- control_enrollments + experiment_enrollments
payments <- control_payments + experiment_payments
```

```{r, GS}
GC_control <- control_enrollments / control_clicks
GC_experimet <- experiment_enrollments / experiment_clicks
GC_total <- enrollments_paid / clicks_paid
GC_SD <- sqrt(GC_total * (1 - GC_total)*((1/control_clicks) + (1/experiment_clicks)))
z <- 1.96
GC_m <- GC_SD * z
GC_d_hat <- GC_experimet - GC_control
GC_CI_low <- GC_d_hat - GC_m
GC_CI_high <- GC_d_hat + GC_m 
```

```{r, printing_GC_results}
cat(paste('Control group: ', round(GC_control,digits = 4)))
cat(paste('Experiment group: ', round(GC_experimet, digits = 4)))
cat(paste('Both groups: ', round(GC_total, digits = 4)))
cat(paste('Standard deviation: ', round(GC_SD, digits = 4)))

cat(paste('z-score (95% CI): ', z))
cat(paste('margin of error: ', round(GC_m,digits = 4)))
cat(paste('Observed difference: ', round(GC_d_hat, digits = 4)))
cat(paste('CI (95%): [',round(GC_CI_low, digits = 4) , ',',round(GC_CI_high,digits = 4),']'))
```

CI does not contain zero, it is statistically significant.
CI does not contain d_min, it is practically significant.

<br>

• Confidence Interval for __Net Conversion__: 
```{r, NC}
NC_control <- control_payments / control_clicks
NC_experimet <- experiment_payments / experiment_clicks
NC_total <- payments / clicks_paid
NC_SD <- sqrt(NC_total * (1 - NC_total)*((1/control_clicks) + (1/experiment_clicks)))
z <- 1.96
NC_m <- NC_SD * z
NC_d_hat <- NC_experimet - NC_control
NC_CI_low <- NC_d_hat - NC_m
NC_CI_high <- NC_d_hat + NC_m 
```

```{r, printing_NC_results}
cat(paste('Control group: ', round(NC_control,digits = 4)))
cat(paste('Experiment group: ', round(NC_experimet, digits = 4)))
cat(paste('Both groups: ', round(NC_SD, digits = 4)))
cat(paste('Standard deviation: ', round(GC_SD, digits = 4)))

cat(paste('z-score (95% CI): ', z))
cat(paste('margin of error: ', round(NC_m,digits = 4)))
cat(paste('Observed difference: ', round(NC_d_hat, digits = 4)))
cat(paste('CI (95%): [',round(NC_CI_low, digits = 4) , ',',round(NC_CI_high,digits = 4),']'))
```

CI contains zero, it is not statistically significant.
CI does contain d_min, it is not practically significant.

<br>

##### 7.2 Sign Tests
A sign test done using the day-by-day data, and the p-value of the sign test is calculated using online tool:
http://graphpad.com/quickcalcs/binomial1.cfm

<br>

• The results for __Gross Conversion__:

Number of "successes": 4 

Number of trials (or subjects) per experiment: 23 

If the probability of "success" in each trial or subject is 0.500, then:

The two-tail P value is 0.0026 !

This is the chance of observing either 4 or fewer successes, or 19 or more successes, in 23 trials.

P-value is less than 0.05 indicates statistially significant of  __Gross Conversion__ .

<br>

• The results for __Net Conversion__:

Number of "successes": 13 

Number of trials (or subjects) per experiment: 23 

If the probability of "success" in each trial or subject is 0.500, then:

The two-tail P value is 0.6776 !

This is the chance of observing either 13 or more successes, or 10 or fewer successes, in 23 trials.

P-value is more than 0.05, which indicates statistially not significant of  __Net Conversion__ .

<br>

##### 7.3 Summary

In this experiment, the control group is enrolled in a free trial for the paid version of the course after they click "start free trial" button. The experiment group received a screener promting them to enter the weekly time, they can commit to the course. The unit of diversion is cookie (pageviews), the invariant metrics, are cookies and then number of clicks. The evaluation metrics are __Gross Conversion__ and __Net Conversion__.

The Bonferroni correction has not been used because for the proposed hypothesis for this experiment both evaluation metrics would had to show statistical significance in order to launch the change, that is  __Gross Conversion__ decrease __and__ the __Net Conversion__ stay at the same level or increase. The Bonferroni correction would recomend the change if __any__ metric would show the statistical significance.

The final analysis revealed that the  __Gross Conversion__ was statistically significant, and therefore rejection of the null hypothesis, but __Net Conversion__ was not statistically significant. Both effect size tests and sign tests supported these conclusions. The experiment did not show strong evidence that the screener successfully diverted the students who would not remain past the free trial period, or the students who were taking advantage of the free trial for 14 days and canceling their subscription before making the first payment, or that Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.

<br>

#### <span style="color:#596479">8. RECOMENDATION</span>

I would not recommend to launch the screener, because __Net Conversion__ was not statistically significant and failed to reject the null hypothesis. The experiment appeared to decrease overall enrollment, apparent in the statistically significant __Gross Conversion__ rate, but was unable to filter out students who would not fully commit to the course past the free trial period and follow through to course completion. 

The __Net conversion__ confidence interval contained the negative of the practical significance boundary and showed a decrease in the experiment group for number of users who remained past the free trial period and made the first payment relative to the number of cookies who clicked the “start free trial” button. This shows negative business impact because some students in the experiment group who may have gone on to make that first payment are canceling before the free trial period expires.

I would recommend to design a follow-up experiment, which might give us a better way to filter out the users who will be more happier, complete the course and use the coaching resources provided by Udacity.

<br>

## <span style="color:purple">Follow-Up Experiment</span>

For a follow-up experiment, I would not worry about how much time users had available to devote to the course. A/B Testing course material is heavily based on the knowledge of the statistics. My suggestion would be to use a quick test with questions asking if a user completed the statistics course, and check the knowledge of some basics of the statistics. Since it is a first course in the DAND, probably completed long time ago, or some users might be working not in the sequence suggested by Udacity Nanocourse developers.

I probably would not change the setup very much.  After the users click the “start free trial” button, they would be presented with a test. If users do not perform well, they would be given 3 choices -  to proceed and complete the checkout and enroll in the free trial, suggestion to access course materials, or a link to brush up on  Statistics course.

Null Hypothesis: no difference in evaluation metrics between the control group and experiment group.

Unit of Diversion: cookie, tracked by user-id after enrollment in the paid version of the course. 

Invariant Metrics: __Number of Cookies__, __Number of Clicks__ on “start free trial button”.

Evaluation Metrics: __Gross Conversion__, __Net Conversion__.

